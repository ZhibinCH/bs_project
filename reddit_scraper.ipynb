{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import praw\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping r/solana...\n",
      "  Day 1: 2025-03-30 (Target: 100 posts)\n",
      "    → Collected 1 (Total: 1)\n",
      "  Finished 2025-03-30: 1 posts\n",
      "  Day 2: 2025-03-29 (Target: 100 posts)\n",
      "    → Collected 4 (Total: 4)\n",
      "  Finished 2025-03-29: 4 posts\n",
      "  Day 3: 2025-03-28 (Target: 100 posts)\n",
      "    → Collected 10 (Total: 10)\n",
      "  Finished 2025-03-28: 10 posts\n",
      "  Day 4: 2025-03-27 (Target: 100 posts)\n",
      "    → Collected 16 (Total: 16)\n",
      "  Finished 2025-03-27: 16 posts\n",
      "  Day 5: 2025-03-26 (Target: 100 posts)\n",
      "    → Collected 22 (Total: 22)\n",
      "  Finished 2025-03-26: 22 posts\n",
      "  Day 6: 2025-03-25 (Target: 100 posts)\n",
      "    → Collected 33 (Total: 33)\n",
      "  Finished 2025-03-25: 33 posts\n",
      "  Day 7: 2025-03-24 (Target: 100 posts)\n",
      "    → Collected 37 (Total: 37)\n",
      "  Finished 2025-03-24: 37 posts\n",
      "\n",
      "Scraping r/cryptocurrency...\n",
      "  Day 1: 2025-03-30 (Target: 100 posts)\n",
      "  Finished 2025-03-30: 0 posts\n",
      "  Day 2: 2025-03-29 (Target: 100 posts)\n",
      "    → Collected 1 (Total: 1)\n",
      "  Finished 2025-03-29: 1 posts\n",
      "  Day 3: 2025-03-28 (Target: 100 posts)\n",
      "    → Collected 1 (Total: 1)\n",
      "  Finished 2025-03-28: 1 posts\n",
      "  Day 4: 2025-03-27 (Target: 100 posts)\n",
      "    → Collected 5 (Total: 5)\n",
      "  Finished 2025-03-27: 5 posts\n",
      "  Day 5: 2025-03-26 (Target: 100 posts)\n",
      "    → Collected 7 (Total: 7)\n",
      "  Finished 2025-03-26: 7 posts\n",
      "  Day 6: 2025-03-25 (Target: 100 posts)\n",
      "    → Collected 8 (Total: 8)\n",
      "  Finished 2025-03-25: 8 posts\n",
      "  Day 7: 2025-03-24 (Target: 100 posts)\n",
      "    → Collected 8 (Total: 8)\n",
      "  Finished 2025-03-24: 8 posts\n",
      "\n",
      "Scraping r/CryptoMarkets...\n",
      "  Day 1: 2025-03-30 (Target: 100 posts)\n",
      "    → Collected 2 (Total: 2)\n",
      "  Finished 2025-03-30: 2 posts\n",
      "  Day 2: 2025-03-29 (Target: 100 posts)\n",
      "    → Collected 3 (Total: 3)\n",
      "  Finished 2025-03-29: 3 posts\n",
      "  Day 3: 2025-03-28 (Target: 100 posts)\n",
      "    → Collected 3 (Total: 3)\n",
      "  Finished 2025-03-28: 3 posts\n",
      "  Day 4: 2025-03-27 (Target: 100 posts)\n",
      "    → Collected 5 (Total: 5)\n",
      "  Finished 2025-03-27: 5 posts\n",
      "  Day 5: 2025-03-26 (Target: 100 posts)\n",
      "    → Collected 6 (Total: 6)\n",
      "  Finished 2025-03-26: 6 posts\n",
      "  Day 6: 2025-03-25 (Target: 100 posts)\n",
      "    → Collected 7 (Total: 7)\n",
      "  Finished 2025-03-25: 7 posts\n",
      "  Day 7: 2025-03-24 (Target: 100 posts)\n",
      "    → Collected 9 (Total: 9)\n",
      "  Finished 2025-03-24: 9 posts\n",
      "\n",
      "Report:\n",
      "Total posts collected: 188\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv() \n",
    "today = datetime.now().strftime(\"%Y%m%d\")\n",
    "# Configuration Dictionary - Centralized settings for the scraper\n",
    "CONFIG = {\n",
    "    # Reddit API credentials (loaded from environment variables)\n",
    "    \"reddit_auth\": {\n",
    "        \"client_id\": os.getenv('REDDIT_CLIENT_ID'),          \n",
    "        \"client_secret\": os.getenv('REDDIT_CLIENT_SECRET'),  \n",
    "        \"user_agent\": os.getenv('REDDIT_USER_AGENT')         \n",
    "    },\n",
    "    \n",
    "    # Scraping parameters\n",
    "    \"scrape_params\": {\n",
    "        \"subreddits\": [\"solana\", \"cryptocurrency\", \"CryptoMarkets\"],  # Target communities\n",
    "        \"search_query\": '(selftext:Solana OR selftext:SOL) AND self:yes',                  # Search keywords\n",
    "        \"posts_per_day\": 100,                             # Target posts per day per subreddit\n",
    "        \"days_to_scrape\": 7,                              # Number of past days to scrape\n",
    "        \"requests_delay\": 2,                              # Delay between API calls (avoid rate limits)\n",
    "        'max_submissions_per_subreddit': 100,             # Max posts to scrape per subreddit\n",
    "        \"sort_by\": \"new\",                                 # Sorting order for posts\n",
    "    },\n",
    "    \n",
    "    # Output configuration\n",
    "    \"output\": {\n",
    "        \"filename\": f\"outputs/solana_reddit_{today}.csv\",          # Output CSV filename\n",
    "    }\n",
    "}\n",
    "\n",
    "def scrape_daily_posts(config):\n",
    "    \n",
    "    # Initialize Reddit API client\n",
    "    reddit = praw.Reddit(**config[\"reddit_auth\"])\n",
    "    all_posts = []  # Master list to store all collected posts\n",
    "    \n",
    "    # Loop through each subreddit in the configuration\n",
    "    for subreddit in config[\"scrape_params\"][\"subreddits\"]:\n",
    "        print(f\"\\nScraping r/{subreddit}...\")\n",
    "        \n",
    "        # Process each day in the requested time range\n",
    "        for day_offset in range(config[\"scrape_params\"][\"days_to_scrape\"]):\n",
    "            # day_offset: #days to go back from today (0 = today, 1 = yesterday, etc.)\n",
    "            # Calculate day window (UTC)\n",
    "            day_end = datetime.now() - timedelta(days=day_offset)\n",
    "            day_start = day_end - timedelta(days=1)\n",
    "            day_str = day_end.strftime('%Y-%m-%d')  # Format as YYYY-MM-DD\n",
    "            \n",
    "            print(f\"  Day {day_offset+1}: {day_str} (Target: {config['scrape_params']['posts_per_day']} posts)\")\n",
    "            \n",
    "            day_posts = []  # Stores posts for current day\n",
    "            last_utc = None  # Tracks pagination anchor\n",
    "            \n",
    "            # Continue scraping until we reach target or run out of posts\n",
    "            while len(day_posts) < config[\"scrape_params\"][\"posts_per_day\"]:\n",
    "                try:\n",
    "                    # We use 'after' and 'before' to limit the search based on sorting order is NEWESR first\n",
    "                    params = {\n",
    "                        'after': int(day_start.timestamp()),  # Start of day (Unix timestamp), posts should not be older than this\n",
    "                        'before': int(last_utc) if last_utc else int(day_end.timestamp()) \n",
    "                        # Paginate backward through time, ensuring no gaps or duplicates between batches\n",
    "                        # On first request: day_end timestamp, creating a window for the first batch\n",
    "                        # Subsequent requests: last_utc timestamp, i.e. the oldest post from previous batch\n",
    "                    }\n",
    "                    \n",
    "                    # Execute search with current parameters\n",
    "                    submissions = reddit.subreddit(subreddit).search(\n",
    "                        config[\"scrape_params\"][\"search_query\"],\n",
    "                        sort=config[\"scrape_params\"]['sort_by'],      \n",
    "                        limit=config[\"scrape_params\"]['max_submissions_per_subreddit'],  # Max posts per request (Reddit limit)\n",
    "                        params=params     # Time filters\n",
    "                    )\n",
    "                    \n",
    "                    # Process batch of submissions\n",
    "                    batch = []\n",
    "                    for submission in submissions:\n",
    "                        # Safety check for post date, skip if older than the start of the day\n",
    "                        if submission.created_utc < day_start.timestamp():\n",
    "                            continue\n",
    "                        # Double-check it's a text post (even though we filtered in search)\n",
    "                        if not submission.is_self:  # is_self=True means text post\n",
    "                            continue\n",
    "                            \n",
    "                        # Additional quality filters for text posts\n",
    "                        if len(submission.selftext) < 50:  # Minimum 50 characters\n",
    "                            continue\n",
    "                            \n",
    "                        # Extract relevant fields\n",
    "                        batch.append({\n",
    "                            'id': submission.id,        # Unique Reddit post ID\n",
    "                            'title': submission.title,  # Post title text\n",
    "                            'content': submission.selftext,  # Main post content/text (empty for link posts)\n",
    "                            'author': str(submission.author),  # Author's username, str() in case author is deleted: \"None\"\n",
    "                            'subreddit': subreddit,     # Which subreddit this comes from\n",
    "                            'upvotes': submission.ups,      # Raw upvote count\n",
    "                            'downvotes': submission.downs,  # Raw downvote count\n",
    "                            'score': submission.score,      # Net upvotes (upvotes - downvotes)\n",
    "                            'upvote_ratio': submission.upvote_ratio,  # Percentage of upvotes (0-1)\n",
    "                            'num_comments': submission.num_comments,  # Total number of comments\n",
    "                            'created_utc': submission.created_utc,    # Creation time (Unix timestamp UTC)\n",
    "                            'url': submission.url,          # URL to the post or external link\n",
    "                            'is_self': submission.is_self,  # Boolean - True: text post, False: link post\n",
    "                            'word_count': len(submission.selftext.split()),  # Add word count\n",
    "                            'is_media': False,  # Explicitly mark as text\n",
    "\n",
    "                            # Additional potentially useful attributes:\n",
    "                            'flair': submission.link_flair_text,  # Post flair text predefined by moderators or users\n",
    "                            'stickied': submission.stickied,      # Boolean if the post is pinned to the top of the subreddit by moderators\n",
    "                            'over_18': submission.over_18,        # Boolean if NSFW marked\n",
    "                            'spoiler': submission.spoiler,        # Boolean if spoiler marked\n",
    "                            'locked': submission.locked,          # Boolean if comments are disabled (no one can reply)\n",
    "                            'distinguished': submission.distinguished  # \"moderator\", \"admin\", or None (normal user post)\n",
    "                        })\n",
    "                        last_utc = submission.created_utc  # Update pagination anchor\n",
    "                    \n",
    "                    # Exit if no posts found\n",
    "                    if not batch:\n",
    "                        break\n",
    "                    \n",
    "                    # Add batch to daily collection\n",
    "                    day_posts.extend(batch)\n",
    "                    print(f\"    → Collected {len(batch)} (Total: {len(day_posts)})\")\n",
    "                    \n",
    "                    # Stop if we've reached the end of available posts\n",
    "                    if len(batch) < config[\"scrape_params\"]['max_submissions_per_subreddit']:\n",
    "                        break\n",
    "                        \n",
    "                    # Respect API rate limits\n",
    "                    time.sleep(config[\"scrape_params\"][\"requests_delay\"])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error: {str(e)}\")\n",
    "                    time.sleep(10)  # Longer delay on error\n",
    "                    continue\n",
    "            \n",
    "            # Trim to exact target if we collected extra\n",
    "            collected_posts = day_posts[:config[\"scrape_params\"][\"posts_per_day\"]]\n",
    "            all_posts.extend(collected_posts)\n",
    "            print(f\"  Finished {day_str}: {len(collected_posts)} posts\")\n",
    "    \n",
    "    # Convert to DataFrame and keep only specified columns\n",
    "    return pd.DataFrame(all_posts)#[config[\"output\"][\"fields\"]]\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Run scraper with configuration\n",
    "    df = scrape_daily_posts(CONFIG)\n",
    "    \n",
    "    # Add human-readable datetime column\n",
    "    df['created_date'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(CONFIG[\"output\"][\"filename\"], index=False)\n",
    "    \n",
    "    # Print summary report\n",
    "    print(f\"\\nReport:\")\n",
    "    print(f\"Total posts collected: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10 posts...\n",
      "Evaluation error: invalid literal for int() with base 10: ''\n",
      "Saved 10 tweets to outputs/evaluated_reddit_20250330.csv\n",
      "Evaluated tweet dataframe has shape: (10, 30)\n"
     ]
    }
   ],
   "source": [
    "# Local application imports\n",
    "from evaluator import evaluate_dataframe\n",
    "from utils import save_to_csv\n",
    "\n",
    "evaluated_reddit = evaluate_dataframe(df[:10])\n",
    "evaluated_reddit_df = save_to_csv(evaluated_reddit, filename_prefix=\"evaluated_reddit\")\n",
    "print(f\"Evaluated tweet dataframe has shape: {evaluated_reddit_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solana_scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
